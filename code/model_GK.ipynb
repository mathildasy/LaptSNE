{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;\n",
    "from scipy.spatial import distance as dist\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "\n",
    "################ visualization function ################ \n",
    "def categorical_scatter_2d(X2D, class_idxs, ms=3, ax=None, alpha=0.1, legend=True, figsize=None, show=False,    savename=None):\n",
    "    ## Plot a 2D matrix with corresponding class labels: each class diff colour\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "    #ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "    classes = list(np.unique(class_idxs))\n",
    "    markers = 'os' * len(classes)\n",
    "    colors = plt.cm.rainbow(np.linspace(0,1,len(classes)))\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        mark = markers[i]\n",
    "        ax.plot(X2D[class_idxs==cls, 0], X2D[class_idxs==cls, 1], marker=mark, \n",
    "            linestyle='', ms=ms, label=str(cls), alpha=alpha, color=colors[i],\n",
    "            markeredgecolor='black', markeredgewidth=0.4)\n",
    "    if legend:\n",
    "        ax.legend()\n",
    "        \n",
    "    if savename is not None:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(savename)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################  definitions: ################ \n",
    "def perplexity(distances, sigmas):\n",
    "    \n",
    "    \"\"\"Wrapper function for quick calculation of \n",
    "    perplexity over a distance matrix.\"\"\"\n",
    "    return calc_perplexity(calc_prob_matrix(distances, sigmas))\n",
    "\n",
    "def softmax(X, diag_zero=True):\n",
    "    \"\"\"Take softmax of each row of matrix X.\"\"\"\n",
    "\n",
    "    # Subtract max for numerical stability\n",
    "    e_x = np.exp(X - np.max(X, axis=1).reshape([-1, 1]))\n",
    "\n",
    "    # We usually want diagonal probailities to be 0.\n",
    "    if diag_zero:\n",
    "        np.fill_diagonal(e_x, 0.)\n",
    "\n",
    "    # Add a tiny constant for stability of log we take later\n",
    "    e_x = e_x + 1e-8  # numerical stability\n",
    "\n",
    "    return e_x / e_x.sum(axis=1).reshape([-1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################  calculation functions ################ \n",
    "def cal_euclidean_dis(df):\n",
    "    '''\n",
    "    input: \n",
    "            df - a two dim array, (num of samples, num of features)\n",
    "    output: \n",
    "            dis - a two dim array, (num of sample, num of sample)\n",
    "    '''\n",
    "    dis = dist.pdist(df,'sqeuclidean')\n",
    "    dis = dist.squareform(dis)\n",
    "    \n",
    "    return dis\n",
    "\n",
    "def calc_perplexity(prob_matrix):\n",
    "    \"\"\"Calculate the perplexity of each row \n",
    "    of a matrix of probabilities.\"\"\"\n",
    "    entropy = -np.sum(prob_matrix * np.log2(prob_matrix), 1)\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n",
    "\n",
    "def calc_prob_matrix(distances, sigmas=None):\n",
    "    \"\"\"Convert a distances matrix to a matrix of probabilities.\"\"\"\n",
    "    if sigmas is not None:\n",
    "        two_sig_sq = 2. * np.square(sigmas.reshape((-1, 1)))\n",
    "        return softmax(distances / two_sig_sq)\n",
    "    else:\n",
    "        return softmax(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################  sub-algorithms ################ \n",
    "def binary_search(eval_fn, target, tol=1e-10, max_iter=10000, lower=1e-20, upper=1000.):\n",
    "    \"\"\"Perform a binary search over input values to eval_fn.\n",
    "    \n",
    "    # Arguments\n",
    "        eval_fn: Function that we are optimising over.\n",
    "        target: Target value we want the function to output.\n",
    "        tol: Float, once our guess is this close to target, stop.\n",
    "        max_iter: Integer, maximum num. iterations to search for.\n",
    "        lower: Float, lower bound of search range.\n",
    "        upper: Float, upper bound of search range.\n",
    "    # Returns:\n",
    "        Float, best input value to function found during search.\n",
    "    \"\"\"\n",
    "    for i in range(max_iter):\n",
    "        guess = (lower + upper) / 2.\n",
    "        val = eval_fn(guess)\n",
    "        if val > target:\n",
    "            upper = guess\n",
    "        else:\n",
    "            lower = guess\n",
    "        if np.abs(val - target) <= tol:\n",
    "            break\n",
    "    return guess\n",
    "\n",
    "def find_optimal_sigmas(distances, target_perplexity):\n",
    "    \"\"\"For each row of distances matrix, find sigma that results\n",
    "    in target perplexity for that role.\"\"\"\n",
    "    sigmas = [] \n",
    "    # For each row of the matrix (each point in our dataset)\n",
    "    for i in range(distances.shape[0]):\n",
    "        # Make fn that returns perplexity of this row given sigma\n",
    "        eval_fn = lambda sigma: \\\n",
    "            perplexity(distances[i:i+1, :], np.array(sigma))\n",
    "        # Binary search over sigmas to achieve target perplexity\n",
    "        correct_sigma = binary_search(eval_fn, target_perplexity)\n",
    "        # Append the resulting sigma to our output array\n",
    "        sigmas.append(correct_sigma)\n",
    "    return np.array(sigmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################  generate probablity ################ \n",
    "def p_conditional_to_joint(P):\n",
    "    \"\"\"Given conditional probabilities matrix P, return\n",
    "    approximation of joint distribution probabilities.\"\"\"\n",
    "    return (P + P.T) / (2. * P.shape[0])\n",
    "\n",
    "def p_joint(X, target_perplexity):\n",
    "    \"\"\"Given a data matrix X, gives joint probabilities matrix.\n",
    "\n",
    "    # Arguments\n",
    "        X: Input data matrix.\n",
    "    # Returns:\n",
    "        P: Matrix with entries p_ij = joint probabilities.\n",
    "    \"\"\"\n",
    "    # Get the negative euclidian distances matrix for our data\n",
    "    distances = cal_euclidean_dis(X)\n",
    "    # Find optimal sigma for each row of this distances matrix\n",
    "    sigmas = find_optimal_sigmas(- distances, target_perplexity)\n",
    "    # Calculate the probabilities based on these optimal sigmas\n",
    "    p_conditional = calc_prob_matrix(- distances, sigmas)\n",
    "    # Go from conditional to joint probabilities matrix\n",
    "    P = p_conditional_to_joint(p_conditional)\n",
    "    return P, sigmas\n",
    "\n",
    "def q_tsne(Y):\n",
    "    \"\"\"\n",
    "    t-SNE: Given low-dimensional representations Y, \n",
    "    compute matrix of joint probabilities with entries q_ij.\n",
    "    \"\"\"\n",
    "    distances = cal_euclidean_dis(Y)\n",
    "    inv_distances = np.power(1. + distances, -1)\n",
    "    np.fill_diagonal(inv_distances, 0.)\n",
    "    return inv_distances / np.sum(inv_distances), inv_distances\n",
    "\n",
    "def q_joint(Y):\n",
    "    dis = dist.pdist(Y)\n",
    "    dis = dist.squareform(dis)\n",
    "    np.fill_diagonal(dis, 1e10)\n",
    "    Q = np.power(dis, -1)\n",
    "    np.fill_diagonal(Q, 0.)\n",
    "    return Q, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################  calculate gradients ################ \n",
    "def expand(X,n,d=2):\n",
    "    n0 = X.shape[0]\n",
    "    n1 = X.shape[1]\n",
    "    return np.tile(X.reshape(n0,n1,1,1),(n,d))\n",
    "\n",
    "# def cal_gy_Q(Q, Y, inv_distances):\n",
    "#     n, d = Y.shape[0], Y.shape[1]\n",
    "#     Y_ = np.tile(Y.reshape(n,1,d),(n,1))\n",
    "#     Y_2 = np.tile(Y,(n,1)).reshape(n,n,d)\n",
    "#     diff_Y = Y_ - Y_2\n",
    "#     gy_Q = (expand(Q * inv_distances, 1, d) * diff_Y.reshape(n,n,1,d)).sum(axis = 1)\n",
    "#     gy_Q = np.kron(Q.reshape(n,n,1),gy_Q.reshape(n,d)).reshape(n,n,n,d)\n",
    "    \n",
    "#     #inv_distances * (diff_Y)\n",
    "#     return gy_Q \n",
    "def get_K(Y, sigmas):\n",
    "    sigma2 = np.mean(sigmas)\n",
    "    eucdis = cal_euclidean_dis(Y)\n",
    "    K = np.exp(-0.5*eucdis/sigma2)\n",
    "    return K,sigma2\n",
    "\n",
    "\n",
    "def cal_gy_K(gK_L, Y, Kval, sigma2):\n",
    "    Y = Y.T\n",
    "    T=gK_L * Kval;\n",
    "    C=np.tile(sum(T),(len(Y),1));\n",
    "    g=2/sigma2*(Y@T-Y*C).T;\n",
    "    return g\n",
    "\n",
    "def power_diag(D,power):\n",
    "    D_new = np.diag(np.power(np.diag(D),power))\n",
    "    return D_new\n",
    "\n",
    "def eigen_grad(H, Q, Y, inv_distances, beta, rho,sigmas):\n",
    "#     gy_Q = cal_gy_Q(Q, Y, inv_distances)\n",
    "    Kval,sigma2 = get_K(Y,sigmas)\n",
    "    n, d = Y.shape[0], Y.shape[1]\n",
    "    D = np.diag(Q.sum(axis = 0))\n",
    "    L = np.eye(n) - power_diag(D,-0.5) @ Q @ power_diag(D,-0.5)\n",
    "    \n",
    "    U0 = -0.5 * power_diag(D,-1.5) @ Q @ power_diag(D,-0.5)\n",
    "    U1 = -0.5 * power_diag(D,-0.5) @ Q @ power_diag(D,-1.5)\n",
    "    ones = np.ones((n,1))\n",
    "    U0 = (U0 @ ones * (H-L) @ ones) @ ones.T\n",
    "    U1 = ones @ ((ones.T @ U0) * (ones.T @ (H-L)))\n",
    "    D = (H-L) * power_diag(D, -1)\n",
    "    grad_LK = U0+U1+D\n",
    "    grad_Y = cal_gy_K(grad_LK, Y, Kval, sigma2)\n",
    "#     U0_ = expand((U0 @ ones * (H-L) @ ones) @ ones.T, n, d) * gy_Q\n",
    "#     U1_ = expand(ones @ ((ones.T @ U0) * (ones.T @ (H-L))), n, d) * gy_Q\n",
    "#     D_ =  expand((H-L) * power_diag(D, -1), n, d) * gy_Q\n",
    "#     grad_Y = sum(sum(U0_ + U1_ + D_))\n",
    "    \n",
    "    lam, eig_V = np.linalg.eig(L)\n",
    "    grad_H = beta * eig_V @ eig_V.T + rho * (H - L)\n",
    "\n",
    "    print('----------- the 5 smallest eigenvalues ---------')\n",
    "    print(lam[:5])\n",
    "\n",
    "    return grad_Y, grad_H\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_grad(P, Q, Y, inv_distances, H, sigmas, beta = 0, rho = 0):\n",
    "    \"\"\"\n",
    "    Estimate the gradient of t-SNE cost with respect to Y.\n",
    "    \"\"\"\n",
    "    pq_diff = P - Q\n",
    "    pq_expanded = np.expand_dims(pq_diff, 2)\n",
    "    y_diffs = np.expand_dims(Y, 1) - np.expand_dims(Y, 0)\n",
    "\n",
    "    # Expand our inv_distances matrix so can multiply by y_diffs\n",
    "    distances_expanded = np.expand_dims(inv_distances, 2)\n",
    "\n",
    "    # Multiply this by inverse distances matrix\n",
    "    y_diffs_wt = y_diffs * distances_expanded\n",
    "\n",
    "    # Multiply then sum over j's\n",
    "    if beta != 0: \n",
    "        grad_Y2, grad_H = eigen_grad(H, Q, Y, inv_distances, beta, rho,sigmas)\n",
    "        \n",
    "        grad_Y = 4. * (pq_expanded * y_diffs_wt).sum(1) + rho * grad_Y2\n",
    "    else:\n",
    "        grad_Y = 4. * (pq_expanded * y_diffs_wt).sum(1)\n",
    "        grad_H = None\n",
    "\n",
    "    print('-------gradient of Y -------')\n",
    "    print(grad_Y[0])\n",
    "\n",
    "    #print('-------gradient of H -------')\n",
    "    #print(grad_H[0])\n",
    "\n",
    "    return grad_Y, grad_H\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yy_grad(P, Q, Y, distances):\n",
    "    \"\"\"\n",
    "    Estimate the gradient of t-SNE cost with respect to Y.\n",
    "    \"\"\"\n",
    "    \n",
    "        #尺度： y'y\n",
    "    #    grad = -2 * (P - Q) * P * P @ Y\n",
    "        #grad = 2 * np.multiply((P - Q) ,abs(P - np.ones_like(P)/2)) @ Y\n",
    "    #     grad = - 2 * np.multiply(np.multiply((P - Q),(P + 0.5 * 1/P)),(P + 0.5 * 1/P)) @ Y\n",
    "    #    grad = - 2 * (P - Q) *(P + 0.5 * 1/P) * (P + 0.5 * 1/P) @ Y\n",
    "        \n",
    "        ##改变尺度: (yi - yj)'(yi - yj)\n",
    "    #     pq_expanded = np.expand_dims(P - Q, 2)\n",
    "    #     y_diffs = np.expand_dims(Y, 1) - np.expand_dims(Y, 0)\n",
    "    #     y_diffs_wt = y_diffs * np.power((np.expand_dims(P, 2) + np.expand_dims(P, -1)),2)\n",
    "    #     grad = -2. * (pq_expanded * y_diffs_wt).sum(1)\n",
    "\n",
    "    pq_expanded = np.expand_dims(P - Q, 2) * np.power((np.expand_dims(P, 2) + 0.0009 * np.expand_dims(P, -1)),2)\n",
    "    #     pq_expanded = np.expand_dims(P - Q, 2) * np.power(np.expand_dims(P, 2),2)\n",
    "    \n",
    "    y_diffs = np.expand_dims(Y, 1) - np.expand_dims(Y, 0)\n",
    "    dij = cal_euclidean_dis(Y)\n",
    "    np.fill_diagonal(dij, 1e10)\n",
    "    dij = np.tile(np.power(dij,-2),(y_diffs.shape[-1],1,1)).T\n",
    "    y_diffs_wt = y_diffs * dij\n",
    "    grad = 4. * (pq_expanded * y_diffs_wt).sum(1)\n",
    "    \n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QY_grad(D, Q):\n",
    "    D = 1/(D+1)\n",
    "    Q_2 = Q*Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1, 4, 5], \n",
    "    [-5, 8, 9]]\n",
    "A = np.array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   8,  10],\n",
       "       [-10,  16,  18]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "################  embedding algorithms ################\n",
    "def init_y(sample_num, sigma, low_dim = 2, random_state = 0):\n",
    "    '''\n",
    "    input:\n",
    "            sample_num - a scalar, the number of data\n",
    "            sigma - a scalar, standard deviation of the initial Gaussian distribution\n",
    "            low_dim - (2 by defualt) the low dimension that the data are projected onto\n",
    "            random_state - (0 by default) the random seed\n",
    "    output:\n",
    "            y0 - a array, (num of samples, low_dim)\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    y0 = np.random.normal(0, sigma, size=(sample_num,low_dim))\n",
    "    \n",
    "    return y0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-167-056eaa86e58b>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-167-056eaa86e58b>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    grads_Y, grads_H = grad_fn(P, Q, Y, distances, ,H,sigmas, beta, rho)\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def estimate_sne(X, y, P, num_iters, q_fn, grad_fn, learning_rate1, learning_rate2,momentum, beta, rho, plot,sigmas):\n",
    "    \"\"\"Estimates a SNE model.\n",
    "\n",
    "    # Arguments\n",
    "        X: Input data matrix.\n",
    "        P: Matrix of joint probabilities.\n",
    "        num_iters: Iterations to train for.\n",
    "        q_fn: Function that takes Y and gives Q prob matrix.\n",
    "        plot: How many times to plot during training.\n",
    "    # Returns:\n",
    "        Y: Matrix, low-dimensional representation of X.\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_num = NUM_POINTS\n",
    "    sigma = 1e-2\n",
    "\n",
    "    \n",
    "    # Initialise our 2D representation\n",
    "    Y = init_y(sample_num, sigma)\n",
    "    # Initialise our 2D H = Initial Laplacian Matrix\n",
    "    Q, distances = q_fn(Y)\n",
    "    n, d = Y.shape[0], Y.shape[1]\n",
    "    D = np.diag(Q.sum(axis = 0))\n",
    "    L = np.eye(n) - power_diag(D,-0.5) @ Q @ power_diag(D,-0.5)\n",
    "    H = L\n",
    "    # Initialise past values (used for momentum)\n",
    "    if momentum:\n",
    "        Y_m2 = Y.copy()\n",
    "        Y_m1 = Y.copy()\n",
    "        \n",
    "        H_m2 = H.copy()\n",
    "        H_m1 = H.copy()\n",
    "\n",
    "    # Start gradient descent loop\n",
    "    for i in tqdm(range(num_iters)):\n",
    "\n",
    "        # Estimate gradients with respect to Y\n",
    "        grads_Y, grads_H = grad_fn(P, Q, Y, distances, ,H,sigmas, beta, rho)\n",
    "\n",
    "        # Update Y\n",
    "        Y = Y - learning_rate1 * grads_Y\n",
    "        # Update H\n",
    "        if beta !=0: H = H - learning_rate2 * grads_H\n",
    "        # Get new Q and distances (distances only used for t-SNE)\n",
    "        Q, distances = q_fn(Y)\n",
    "        \n",
    "        if momentum:  # Add momentum\n",
    "            Y += momentum * (Y_m1 - Y_m2)\n",
    "            # Update previous Y's for momentum\n",
    "            Y_m2 = Y_m1.copy()\n",
    "            Y_m1 = Y.copy()\n",
    "            if beta != 0:\n",
    "                H += momentum * (H_m1 - H_m2)\n",
    "                H_m2 = H_m1.copy()\n",
    "                H_m1 = H.copy()\n",
    "        \n",
    "        if plot and i % (num_iters / plot) == 0:\n",
    "            categorical_scatter_2d(Y, y, alpha=1.0, ms=6,\n",
    "                                   show=True, figsize=(9, 6))\n",
    "    return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X,y):\n",
    "    \n",
    "    # Obtain matrix of joint probabilities p_ij\n",
    "    P,sigmas = p_joint(X, PERPLEXITY)\n",
    "    \n",
    "    # Fit SNE or t-SNE\n",
    "    Y = estimate_sne(X, y, P,\n",
    "             num_iters=NUM_ITERS,\n",
    "             q_fn = q_tsne if TSNE else q_joint,\n",
    "             grad_fn = tsne_grad if TSNE_grad else yy_grad, \n",
    "             learning_rate1=LEARNING_RATE[0], \n",
    "             learning_rate2=LEARNING_RATE[1], \n",
    "             momentum=MOMENTUM, \n",
    "             beta = BETA,\n",
    "             rho = RHO,\n",
    "             plot=NUM_PLOTS, sigmas = sigmas)\n",
    "    \n",
    "    return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- the 5 smallest eigenvalues ---------\n",
      "[5.53293328e-17 1.00157272e+00 1.00156616e+00 1.00176286e+00\n",
      " 1.00176269e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (569,) (569,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-ac882010581e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mNUM_PLOTS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0my_tSNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-168-569d080d1c56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Fit SNE or t-SNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     Y = estimate_sne(X, y, P,\n\u001b[0m\u001b[1;32m      8\u001b[0m              \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_ITERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m              \u001b[0mq_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_tsne\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTSNE\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mq_joint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-b67c966b62e7>\u001b[0m in \u001b[0;36mestimate_sne\u001b[0;34m(X, y, P, num_iters, q_fn, grad_fn, learning_rate1, learning_rate2, momentum, beta, rho, plot, sigmas)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Estimate gradients with respect to Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mgrads_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_H\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Update Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-d8ec0b9594ef>\u001b[0m in \u001b[0;36mtsne_grad\u001b[0;34m(P, Q, Y, inv_distances, H, sigmas, beta, rho)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mgrad_Y2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_H\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigen_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mgrad_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpq_expanded\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_diffs_wt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_Y2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgrad_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpq_expanded\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_diffs_wt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (569,) (569,2) "
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_breast_cancer()\n",
    "#data = load_wine()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "NUM_POINTS = X.shape[0]           # Number of samples from Wine\n",
    "PERPLEXITY = 30\n",
    "SEED = 1                    # Random seed\n",
    "MOMENTUM = 0.8\n",
    "# LEARNING_RATE = (1e2, 0)\n",
    "# BETA = 0\n",
    "# RHO = 0\n",
    "LEARNING_RATE = (1e2, 1e0)\n",
    "BETA = 1e-1\n",
    "RHO = 1e1\n",
    "NUM_ITERS = 100\n",
    "TSNE = True\n",
    "TSNE_grad = True \n",
    "NUM_PLOTS = 5\n",
    "\n",
    "y_tSNE = main(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
